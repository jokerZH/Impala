From e0991c4e6db23bbc8989fd0f422e986ec589a3cf Mon Sep 17 00:00:00 2001
From: Michael Stack <stack@apache.org>
Date: Mon, 20 Nov 2017 12:31:07 -0800
Subject: [PATCH 443/471] CDH-62141 Backport CDH5.7 MultiTableInputFormatBase
 Connection handling and HBASE-19299 for Cerner

Backports hbase-1.2 (CDH5.7+) Connection handling in MultiTableInputFormatBase#getSplits
to older hbase-1.0x CDHs; e.g. CDH5.5.

M hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java
 Small change where we make the Connection and Admin once up front just
 inside a try. A new finally clause closes them out when done. Change
 looks big but it is just a right-shift of a bunch of code.

M hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormatBase.java
 New test that verifies only one Connection made. This test was
 added upstream as HBASE-19299 Assert only one Connection is constructed
 when calculating splits in a MultiTableInputFormat

Change-Id: Iacdbb18dfc862be04a810adf0e0a0b1896413066
Reason: Bug
Author: stack
Ref: CDH-62141
---
 .../hbase/mapreduce/MultiTableInputFormatBase.java |  108 ++++-----
 .../mapreduce/TestMultiTableInputFormatBase.java   |  240 ++++++++++++++++++++
 2 files changed, 290 insertions(+), 58 deletions(-)
 create mode 100644 hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormatBase.java

diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java
index 4931c3f..fdffc8a 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/MultiTableInputFormatBase.java
@@ -29,6 +29,7 @@ import org.apache.hadoop.hbase.classification.InterfaceStability;
 import org.apache.hadoop.hbase.HRegionInfo;
 import org.apache.hadoop.hbase.HRegionLocation;
 import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.Admin;
 import org.apache.hadoop.hbase.client.Connection;
 import org.apache.hadoop.hbase.client.ConnectionFactory;
 import org.apache.hadoop.hbase.client.RegionLocator;
@@ -45,9 +46,6 @@ import org.apache.hadoop.mapreduce.JobContext;
 import org.apache.hadoop.mapreduce.RecordReader;
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 
-import java.util.Map;
-import java.util.HashMap;
-import java.util.Iterator;
 /**
  * A base for {@link MultiTableInputFormat}s. Receives a list of
  * {@link Scan} instances that define the input tables and
@@ -162,85 +160,79 @@ public abstract class MultiTableInputFormatBase extends
     if (scans.isEmpty()) {
       throw new IOException("No scans were provided.");
     }
-
-    Map<TableName, List<Scan>> tableMaps = new HashMap<TableName, List<Scan>>();
-    for (Scan scan : scans) {
-      byte[] tableNameBytes = scan.getAttribute(Scan.SCAN_ATTRIBUTES_TABLE_NAME);
-      if (tableNameBytes == null)
-        throw new IOException("A scan object did not have a table name");
-
-      TableName tableName = TableName.valueOf(tableNameBytes);
-
-      List<Scan> scanList = tableMaps.get(tableName);
-      if (scanList == null) {
-        scanList = new ArrayList<Scan>();
-        tableMaps.put(tableName, scanList);
-      }
-      scanList.add(scan);
-    }
-
     List<InputSplit> splits = new ArrayList<InputSplit>();
-    Iterator iter = tableMaps.entrySet().iterator();
-    while (iter.hasNext()) {
-      Map.Entry<TableName, List<Scan>> entry = (Map.Entry<TableName, List<Scan>>) iter.next();
-      TableName tableName = entry.getKey();
-      List<Scan> scanList = entry.getValue();
 
-      try (Connection conn = ConnectionFactory.createConnection(context.getConfiguration());
-        Table table = conn.getTable(tableName);
-        RegionLocator regionLocator = conn.getRegionLocator(tableName)) {
-        RegionSizeCalculator sizeCalculator = new RegionSizeCalculator(
-                regionLocator, conn.getAdmin());
-        Pair<byte[][], byte[][]> keys = regionLocator.getStartEndKeys();
-        for (Scan scan : scanList) {
-          if (keys == null || keys.getFirst() == null || keys.getFirst().length == 0) {
+    Connection conn = null;
+    Admin admin = null;
+    try {
+      conn = ConnectionFactory.createConnection(context.getConfiguration());
+      admin = conn.getAdmin();
+      for (Scan scan : scans) {
+        byte[] tableNameBytes = scan.getAttribute(Scan.SCAN_ATTRIBUTES_TABLE_NAME);
+        if (tableNameBytes == null)
+          throw new IOException("A scan object did not have a table name");
+        TableName tableName = TableName.valueOf(tableNameBytes);
+        Table table = null;
+        RegionLocator regionLocator = null;
+        try {
+          table = conn.getTable(tableName);
+          regionLocator = conn.getRegionLocator(tableName);
+          Pair<byte[][], byte[][]> keys = regionLocator.getStartEndKeys();
+          if (keys == null || keys.getFirst() == null ||
+              keys.getFirst().length == 0) {
             throw new IOException("Expecting at least one region for table : "
-                    + tableName.getNameAsString());
+                + tableName.getNameAsString());
           }
           int count = 0;
 
           byte[] startRow = scan.getStartRow();
           byte[] stopRow = scan.getStopRow();
 
+          RegionSizeCalculator sizeCalculator = new RegionSizeCalculator(
+              regionLocator, admin);
+
           for (int i = 0; i < keys.getFirst().length; i++) {
             if (!includeRegionInSplit(keys.getFirst()[i], keys.getSecond()[i])) {
               continue;
             }
+            HRegionLocation hregionLocation = regionLocator.getRegionLocation(
+                keys.getFirst()[i], false);
+            String regionHostname = hregionLocation.getHostname();
+            HRegionInfo regionInfo = hregionLocation.getRegionInfo();
 
+            // determine if the given start and stop keys fall into the range
             if ((startRow.length == 0 || keys.getSecond()[i].length == 0 ||
-                    Bytes.compareTo(startRow, keys.getSecond()[i]) < 0) &&
-                    (stopRow.length == 0 || Bytes.compareTo(stopRow,
-                            keys.getFirst()[i]) > 0)) {
-              byte[] splitStart = startRow.length == 0 ||
-                      Bytes.compareTo(keys.getFirst()[i], startRow) >= 0 ?
-                      keys.getFirst()[i] : startRow;
-              byte[] splitStop = (stopRow.length == 0 ||
-                      Bytes.compareTo(keys.getSecond()[i], stopRow) <= 0) &&
-                      keys.getSecond()[i].length > 0 ?
-                      keys.getSecond()[i] : stopRow;
-
-              HRegionLocation hregionLocation = regionLocator.getRegionLocation(
-                      keys.getFirst()[i], false);
-              String regionHostname = hregionLocation.getHostname();
-              HRegionInfo regionInfo = hregionLocation.getRegionInfo();
-              String encodedRegionName = regionInfo.getEncodedName();
-              long regionSize = sizeCalculator.getRegionSize(
-                      regionInfo.getRegionName());
-
-              TableSplit split = new TableSplit(table.getName(),
-                      scan, splitStart, splitStop, regionHostname,
-                      encodedRegionName, regionSize);
+                Bytes.compareTo(startRow, keys.getSecond()[i]) < 0) &&
+                (stopRow.length == 0 ||
+                    Bytes.compareTo(stopRow, keys.getFirst()[i]) > 0)) {
+              byte[] splitStart =
+                  startRow.length == 0 ||
+                      Bytes.compareTo(keys.getFirst()[i], startRow) >= 0 ? keys
+                      .getFirst()[i] : startRow;
+              byte[] splitStop =
+                  (stopRow.length == 0 || Bytes.compareTo(keys.getSecond()[i],
+                      stopRow) <= 0) && keys.getSecond()[i].length > 0 ? keys
+                      .getSecond()[i] : stopRow;
+
+              long regionSize = sizeCalculator.getRegionSize(regionInfo.getRegionName());
+              TableSplit split =
+                  new TableSplit(regionLocator.getName(),
+                      scan, splitStart, splitStop, regionHostname, regionSize);
 
               splits.add(split);
-
               if (LOG.isDebugEnabled())
                 LOG.debug("getSplits: split -> " + (count++) + " -> " + split);
             }
           }
+        } finally {
+          if (null != table) table.close();
+          if (null != regionLocator) regionLocator.close();
         }
       }
+    } finally {
+      if (null != admin) admin.close();
+      if (null != conn) conn.close();
     }
-
     return splits;
   }
 
diff --git a/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormatBase.java b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormatBase.java
new file mode 100644
index 0000000..d90af60
--- /dev/null
+++ b/hbase-server/src/test/java/org/apache/hadoop/hbase/mapreduce/TestMultiTableInputFormatBase.java
@@ -0,0 +1,240 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hadoop.hbase.mapreduce;
+
+import static org.apache.hadoop.hbase.client.Scan.SCAN_ATTRIBUTES_TABLE_NAME;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.TreeMap;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hbase.CategoryBasedTimeout;
+import org.apache.hadoop.hbase.ClusterStatus;
+import org.apache.hadoop.hbase.HBaseConfiguration;
+import org.apache.hadoop.hbase.HConstants;
+import org.apache.hadoop.hbase.HRegionInfo;
+import org.apache.hadoop.hbase.HRegionLocation;
+import org.apache.hadoop.hbase.ServerName;
+import org.apache.hadoop.hbase.TableName;
+import org.apache.hadoop.hbase.client.Admin;
+import org.apache.hadoop.hbase.client.BufferedMutator;
+import org.apache.hadoop.hbase.client.BufferedMutatorParams;
+import org.apache.hadoop.hbase.client.ClusterConnection;
+import org.apache.hadoop.hbase.client.Connection;
+import org.apache.hadoop.hbase.client.RegionLocator;
+import org.apache.hadoop.hbase.client.Result;
+import org.apache.hadoop.hbase.client.Scan;
+import org.apache.hadoop.hbase.client.Table;
+import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
+import org.apache.hadoop.hbase.security.User;
+import org.apache.hadoop.hbase.testclassification.SmallTests;
+import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hbase.util.Pair;
+import org.apache.hadoop.mapreduce.InputSplit;
+import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.RecordReader;
+import org.apache.hadoop.mapreduce.TaskAttemptContext;
+import org.junit.Assert;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.experimental.categories.Category;
+import org.junit.rules.TestName;
+import org.junit.rules.TestRule;
+import org.mockito.Mockito;
+import org.mockito.invocation.InvocationOnMock;
+import org.mockito.stubbing.Answer;
+
+/**
+ * Tests of MultiTableInputFormatBase.
+ */
+@Category({SmallTests.class})
+public class TestMultiTableInputFormatBase {
+  @Rule public final TestName name = new TestName();
+  @Rule public final TestRule timeout = CategoryBasedTimeout.builder()
+      .withTimeout(this.getClass())
+      .withLookingForStuckThread(true)
+      .build();
+
+  /**
+   * Test getSplits only puts up one Connection.
+   * In past it has put up many Connections. Each Connection setup comes with a fresh new cache
+   * so we have to do fresh hit on hbase:meta. Should only do one Connection when doing getSplits
+   * even if a MultiTableInputFormat.
+   * @throws IOException
+   */
+  @Test
+  public void testMRSplitsConnectionCount() throws IOException {
+    // Make instance of MTIFB.
+    MultiTableInputFormatBase mtif = new MultiTableInputFormatBase() {
+      @Override
+      public RecordReader<ImmutableBytesWritable, Result> createRecordReader(InputSplit split,
+          TaskAttemptContext context)
+      throws IOException, InterruptedException {
+        return super.createRecordReader(split, context);
+      }
+    };
+    // Pass it a mocked JobContext. Make the JC return our Configuration.
+    // Load the Configuration so it returns our special Connection so we can interpolate
+    // canned responses.
+    JobContext mockedJobContext = Mockito.mock(JobContext.class);
+    Configuration c = HBaseConfiguration.create();
+    c.set(ClusterConnection.HBASE_CLIENT_CONNECTION_IMPL, MRSplitsConnection.class.getName());
+    Mockito.when(mockedJobContext.getConfiguration()).thenReturn(c);
+    // Invent a bunch of scans. Have each Scan go against a different table so a good spread.
+    List<Scan> scans = new ArrayList<>();
+    for (int i = 0; i < 10; i++) {
+      Scan scan = new Scan();
+      String tableName = this.name.getMethodName() + i;
+      scan.setAttribute(SCAN_ATTRIBUTES_TABLE_NAME, Bytes.toBytes(tableName));
+      scans.add(scan);
+    }
+    mtif.setScans(scans);
+    // Get splits. Assert that that more than one.
+    List<InputSplit> splits = mtif.getSplits(mockedJobContext);
+    Assert.assertTrue(splits.size() > 0);
+    // Assert only one Connection was made (see the static counter we have in the mocked
+    // Connection MRSplitsConnection Constructor.
+    Assert.assertEquals(1, MRSplitsConnection.creations.get());
+  }
+
+  /**
+   * Connection to use above in Test.
+   */
+  public static class MRSplitsConnection implements Connection {
+    private final Configuration configuration;
+    static final AtomicInteger creations = new AtomicInteger(0);
+
+    MRSplitsConnection (Configuration conf, boolean bool, ExecutorService pool, User user) throws IOException {
+      this.configuration = conf;
+      creations.incrementAndGet();
+    }
+
+    @Override
+    public void abort(String why, Throwable e) {
+
+    }
+
+    @Override
+    public boolean isAborted() {
+      return false;
+    }
+
+    @Override
+    public Configuration getConfiguration() {
+      return this.configuration;
+    }
+
+    @Override
+    public BufferedMutator getBufferedMutator(TableName tableName) throws IOException {
+      return null;
+    }
+
+    @Override
+    public BufferedMutator getBufferedMutator(BufferedMutatorParams params) throws IOException {
+      return null;
+    }
+
+    @Override
+    public RegionLocator getRegionLocator(final TableName tableName) throws IOException {
+      // Make up array of start keys. We start off w/ empty byte array.
+      final byte [][] startKeys = new byte [][] {HConstants.EMPTY_BYTE_ARRAY,
+          Bytes.toBytes("aaaa"), Bytes.toBytes("bbb"),
+          Bytes.toBytes("ccc"), Bytes.toBytes("ddd"), Bytes.toBytes("eee"),
+          Bytes.toBytes("fff"), Bytes.toBytes("ggg"), Bytes.toBytes("hhh"),
+          Bytes.toBytes("iii"), Bytes.toBytes("lll"), Bytes.toBytes("mmm"),
+          Bytes.toBytes("nnn"), Bytes.toBytes("ooo"), Bytes.toBytes("ppp"),
+          Bytes.toBytes("qqq"), Bytes.toBytes("rrr"), Bytes.toBytes("sss"),
+          Bytes.toBytes("ttt"), Bytes.toBytes("uuu"), Bytes.toBytes("vvv"),
+          Bytes.toBytes("zzz")};
+      // Make an array of end keys. We end with the empty byte array.
+      final byte [][] endKeys = new byte[][] {
+          Bytes.toBytes("aaaa"), Bytes.toBytes("bbb"),
+          Bytes.toBytes("ccc"), Bytes.toBytes("ddd"), Bytes.toBytes("eee"),
+          Bytes.toBytes("fff"), Bytes.toBytes("ggg"), Bytes.toBytes("hhh"),
+          Bytes.toBytes("iii"), Bytes.toBytes("lll"), Bytes.toBytes("mmm"),
+          Bytes.toBytes("nnn"), Bytes.toBytes("ooo"), Bytes.toBytes("ppp"),
+          Bytes.toBytes("qqq"), Bytes.toBytes("rrr"), Bytes.toBytes("sss"),
+          Bytes.toBytes("ttt"), Bytes.toBytes("uuu"), Bytes.toBytes("vvv"),
+          Bytes.toBytes("zzz"),
+          HConstants.EMPTY_BYTE_ARRAY};
+      // Now make a map of start keys to HRegionLocations. Let the server namber derive from
+      // the start key.
+      final Map<byte [], HRegionLocation> map =
+          new TreeMap<byte [], HRegionLocation>(Bytes.BYTES_COMPARATOR);
+      for (byte [] startKey: startKeys) {
+        HRegionLocation hrl = new HRegionLocation(new HRegionInfo(tableName, startKey, startKey),
+          ServerName.valueOf(Bytes.toString(startKey), 0, 0));
+        map.put(startKey, hrl);
+      }
+      // Get a list of the locations.
+      final List<HRegionLocation> locations = new ArrayList<HRegionLocation>(map.values());
+      // Now make a RegionLocator mock backed by the abpve map and list of locations.
+      RegionLocator mockedRegionLocator = Mockito.mock(RegionLocator.class);
+      Mockito.when(mockedRegionLocator.getRegionLocation(Mockito.any(byte [].class),
+            Mockito.anyBoolean())).
+          thenAnswer(new Answer<HRegionLocation>() {
+            @Override
+            public HRegionLocation answer(InvocationOnMock invocationOnMock) throws Throwable {
+              Object [] args = invocationOnMock.getArguments();
+              byte [] key = (byte [])args[0];
+              return map.get(key);
+            }
+          });
+      Mockito.when(mockedRegionLocator.getAllRegionLocations()).thenReturn(locations);
+      Mockito.when(mockedRegionLocator.getStartEndKeys()).
+          thenReturn(new Pair<byte [][], byte[][]>(startKeys, endKeys));
+      Mockito.when(mockedRegionLocator.getName()).thenReturn(tableName);
+      return mockedRegionLocator;
+    }
+
+    @Override
+    public Admin getAdmin() throws IOException {
+      Admin admin = Mockito.mock(Admin.class);
+      Mockito.when(admin.getConfiguration()).thenReturn(getConfiguration());
+      Mockito.when(admin.getClusterStatus()).thenReturn(Mockito.mock(ClusterStatus.class));
+      return admin;
+    }
+
+    @Override
+    public Table getTable(TableName tableName) throws IOException {
+      Table table = Mockito.mock(Table.class);
+      Mockito.when(table.getName()).thenReturn(tableName);
+      return table;
+    }
+
+    @Override
+    public Table getTable(TableName tableName, ExecutorService es) throws IOException {
+      return getTable(tableName);
+    }
+
+    @Override
+    public void close() throws IOException {
+
+    }
+
+    @Override
+    public boolean isClosed() {
+      return false;
+    }
+  }
+}
-- 
1.7.9.5

